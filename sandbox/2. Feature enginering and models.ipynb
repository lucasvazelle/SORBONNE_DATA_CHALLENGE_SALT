{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Partie 1 : CodeBERT"
      ],
      "metadata": {
        "id": "EjhDA0fJEN0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pour le trainset"
      ],
      "metadata": {
        "id": "ca8kbkc5GDl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# ========== CONFIGURATION ==========\n",
        "json_folder = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/SDC/Fichiers source/folder_training_set_2/folder_training_set\"\n",
        "save_dir = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/instruction_embeddings_grouped_codebert_v2\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# ========== MODELE ET PERIPHERIQUE ==========\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "model = AutoModel.from_pretrained(\"microsoft/codebert-base\", torch_dtype=torch.float16).to(device)\n",
        "model.eval()\n",
        "\n",
        "# ========== FONCTIONS UTILITAIRES ==========\n",
        "def is_valid_dot_file(filepath):\n",
        "    try:\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            first_line = f.readline().strip().lower()\n",
        "            return first_line and (\"digraph\" in first_line or \"graph\" in first_line)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def extract_labels_from_text(dot_text):\n",
        "    return re.findall(r'label\\s*=\\s*\"(.*?)\"', dot_text)\n",
        "\n",
        "# Version A: Traitement des labels par fréquence\n",
        "def extract_distinct_payloads_version_a(node_labels):\n",
        "    freq = Counter()\n",
        "    cleaned_payloads = []\n",
        "\n",
        "    for label in node_labels:\n",
        "        parts = label.split(\":\")\n",
        "        payload = \":\".join(parts[1:]).strip() if len(parts) > 1 else label.strip()\n",
        "        payload = re.sub(r\"[:,\\\"\\[\\],]\", \" \", payload)\n",
        "        payload = re.sub(r\"[^a-zA-Z0-9_\\-+*/\\\\= ]+\", \" \", payload)\n",
        "        payload = re.sub(r\"\\b0x[0-9a-fA-F]+\\b\", '', payload)\n",
        "        payload = re.sub(r'\\b[a-zA-Z]+\\s*=\\s*0x[0-9a-fA-F]+\\b', '', payload)\n",
        "        payload = re.sub(r\"=\", \"\", payload)\n",
        "        payload = re.sub(r\"[+\\-]\", \"\", payload)\n",
        "        payload = re.sub(r\"\\s+\", \" \", payload).strip()\n",
        "        if payload:\n",
        "            freq[payload] += 1\n",
        "            cleaned_payloads.append(payload)\n",
        "\n",
        "    distinct_ordered = []\n",
        "    seen = set()\n",
        "    for p in sorted(cleaned_payloads, key=lambda x: freq[x]):\n",
        "        if p not in seen:\n",
        "            seen.add(p)\n",
        "            distinct_ordered.append(p)\n",
        "    return distinct_ordered\n",
        "\n",
        "# ========== FONCTIONS D'EMBEDDING ==========\n",
        "def embed_instruction_batch(list_texts):\n",
        "    encodings = tokenizer(list_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    encodings = {k: v.to(device) for k, v in encodings.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encodings)\n",
        "        hidden = outputs.last_hidden_state\n",
        "        attention_mask = encodings[\"attention_mask\"].unsqueeze(-1).expand(hidden.size())\n",
        "        summed = (hidden * attention_mask).sum(dim=1)\n",
        "        counts = attention_mask.sum(dim=1)\n",
        "        mean_pooled = summed / counts\n",
        "    return mean_pooled.cpu()\n",
        "\n",
        "# ========== GESTION DES IDs ==========\n",
        "def get_all_embedded_ids(save_dir):\n",
        "    all_ids = []\n",
        "    for fname in os.listdir(save_dir):\n",
        "        if fname.startswith(\"embedding_\") and fname.endswith(\".pt\"):\n",
        "            try:\n",
        "                data = torch.load(os.path.join(save_dir, fname), map_location=\"cpu\")\n",
        "                all_ids.extend(data.get(\"ids\", []))\n",
        "            except:\n",
        "                continue\n",
        "    return set(all_ids)\n",
        "\n",
        "# ========== ENCODAGE PRINCIPAL ==========\n",
        "def encode_graphs_in_size_range(json_folder, save_dir, min_kb, max_kb, batch_size=32,\n",
        "                                custom_file_list=None, custom_output_name=None):\n",
        "    embedded_ids = get_all_embedded_ids(save_dir)\n",
        "\n",
        "    all_files = [(path, os.path.getsize(path) / 1024.0) for path in custom_file_list] if custom_file_list else []\n",
        "    if not custom_file_list:\n",
        "        for f in os.listdir(json_folder):\n",
        "            if not f.endswith(\".json\"): continue\n",
        "            path = os.path.join(json_folder, f)\n",
        "            size_kb = os.path.getsize(path) / 1024.0\n",
        "            if min_kb <= size_kb < max_kb:\n",
        "                graph_id = f.replace(\".json\", \"\")\n",
        "                if graph_id not in embedded_ids:\n",
        "                    all_files.append((path, size_kb))\n",
        "\n",
        "    all_files.sort(key=lambda x: x[1])\n",
        "    print(f\"Fichiers à traiter ({min_kb}-{max_kb}KB): {len(all_files)}\")\n",
        "\n",
        "    buffer_instr, buffer_ids = [], []\n",
        "    embeddings, ids, failed = [], [], []\n",
        "\n",
        "    for path, size_kb in tqdm(all_files):\n",
        "        if not is_valid_dot_file(path):\n",
        "            failed.append(os.path.basename(path))\n",
        "            continue\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                dot_text = f.read()\n",
        "            labels = extract_labels_from_text(dot_text)\n",
        "            distinct_payloads = extract_distinct_payloads_version_a(labels)\n",
        "            combined_instr = \" \".join(distinct_payloads)[:2560]\n",
        "            file_id = os.path.basename(path).replace(\".json\", \"\")\n",
        "            buffer_instr.append(combined_instr)\n",
        "            buffer_ids.append(file_id)\n",
        "\n",
        "            if len(buffer_instr) >= batch_size:\n",
        "                batch_emb = embed_instruction_batch(buffer_instr)\n",
        "                embeddings.extend(batch_emb)\n",
        "                ids.extend(buffer_ids)\n",
        "                buffer_instr.clear()\n",
        "                buffer_ids.clear()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur avec {os.path.basename(path)}: {e}\")\n",
        "            failed.append(os.path.basename(path))\n",
        "\n",
        "    if buffer_instr:\n",
        "        batch_emb = embed_instruction_batch(buffer_instr)\n",
        "        embeddings.extend(batch_emb)\n",
        "        ids.extend(buffer_ids)\n",
        "\n",
        "    if embeddings:\n",
        "        tensor = torch.stack(embeddings)\n",
        "        output_name = custom_output_name if custom_output_name else f\"embedding_{min_kb}_{max_kb}_KB.pt\"\n",
        "        torch.save({'ids': ids, 'embeddings': tensor}, os.path.join(save_dir, output_name))\n",
        "        print(f\"Sauvegarde de {len(ids)} graphs dans {output_name}\")\n",
        "\n",
        "    if failed:\n",
        "        log_path = os.path.join(save_dir, f\"log_{min_kb}_{max_kb}_KB.json\")\n",
        "        with open(log_path, \"w\") as f:\n",
        "            json.dump({\"failed\": failed, \"timestamp\": datetime.now().isoformat()}, f, indent=2)\n",
        "        failed_txt_path = os.path.join(save_dir, \"failed_graphs_remaining_test.txt\")\n",
        "        with open(failed_txt_path, \"w\") as ftxt:\n",
        "            for fname in failed:\n",
        "                ftxt.write(fname + \"\\n\")\n",
        "        print(f\"Erreurs: {len(failed)} - log dans {log_path}\")\n",
        "\n",
        "# ========== EXECUTION ==========\n",
        "if __name__ == \"__main__\":\n",
        "    max_new_pt_files = 100\n",
        "    created_pt_count = 0\n",
        "    start_group_num = 1\n",
        "\n",
        "    # Liste des fichiers existants\n",
        "    print(\"\\nFichiers .pt existants:\")\n",
        "    pt_files = sorted([f for f in os.listdir(save_dir) if f.endswith(\".pt\")])\n",
        "    for f in pt_files:\n",
        "        f_path = os.path.join(save_dir, f)\n",
        "        try:\n",
        "            data = torch.load(f_path, map_location=\"cpu\")\n",
        "            n_graphs = len(data.get(\"ids\", []))\n",
        "            print(f\"  - {f:<35} contient {n_graphs} graphs\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Erreur lecture {f}: {e}\")\n",
        "\n",
        "    # Fichiers restants à traiter\n",
        "    embedded_ids = get_all_embedded_ids(save_dir)\n",
        "    remaining_files = []\n",
        "    for fname in os.listdir(json_folder):\n",
        "        if fname.endswith(\".json\"):\n",
        "            graph_id = fname.replace(\".json\", \"\")\n",
        "            if graph_id not in embedded_ids:\n",
        "                full_path = os.path.join(json_folder, fname)\n",
        "                size_kb = os.path.getsize(full_path) / 1024.0\n",
        "                remaining_files.append((full_path, size_kb))\n",
        "\n",
        "    print(f\"\\nGraphs restants: {len(remaining_files)}\")\n",
        "    remaining_files.sort(key=lambda x: x[1])\n",
        "\n",
        "    # Traitement par groupes\n",
        "    group_size = 50\n",
        "    for i in range(0, len(remaining_files), group_size):\n",
        "        group = remaining_files[i:i+group_size]\n",
        "        if not group:\n",
        "            continue\n",
        "\n",
        "        group_num = start_group_num + (i // group_size)\n",
        "        output_name = f\"embedding_50_graphs_{group_num:02d}.pt\"\n",
        "        output_path = os.path.join(save_dir, output_name)\n",
        "\n",
        "        if os.path.exists(output_path):\n",
        "            print(f\"Fichier {output_name} existe déjà\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nTraitement groupe {group_num:02d} ({len(group)} fichiers) → {output_name}\")\n",
        "        file_list = [path for path, _ in group]\n",
        "        encode_graphs_in_size_range(\n",
        "            json_folder=json_folder,\n",
        "            save_dir=save_dir,\n",
        "            min_kb=0, max_kb=1_000_000,\n",
        "            batch_size=32,\n",
        "            custom_file_list=file_list,\n",
        "            custom_output_name=output_name\n",
        "        )\n",
        "\n",
        "        created_pt_count += 1\n",
        "        if created_pt_count >= max_new_pt_files:\n",
        "            print(\"Nombre maximal de fichiers atteint\")\n",
        "            break"
      ],
      "metadata": {
        "id": "PKulV2NTETbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pour le testset"
      ],
      "metadata": {
        "id": "-3ba_1o7Gst1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# ========== CONFIGURATION ==========\n",
        "json_folder = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/folder_test_set\"\n",
        "save_dir = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/instruction_embeddings_test_codebert_v2\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# ========== MODELE ET PERIPHERIQUE ==========\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "model = AutoModel.from_pretrained(\"microsoft/codebert-base\", torch_dtype=torch.float16).to(device)\n",
        "model.eval()\n",
        "\n",
        "# ========== FONCTIONS UTILITAIRES ==========\n",
        "def is_valid_dot_file(filepath):\n",
        "    try:\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            first_line = f.readline().strip().lower()\n",
        "            return first_line and (\"digraph\" in first_line or \"graph\" in first_line)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def extract_labels_from_text(dot_text):\n",
        "    return re.findall(r'label\\s*=\\s*\"(.*?)\"', dot_text)\n",
        "\n",
        "# Version A: Traitement des labels par fréquence\n",
        "def extract_distinct_payloads_version_a(node_labels):\n",
        "    freq = Counter()\n",
        "    cleaned_payloads = []\n",
        "\n",
        "    for label in node_labels:\n",
        "        parts = label.split(\":\")\n",
        "        payload = \":\".join(parts[1:]).strip() if len(parts) > 1 else label.strip()\n",
        "        payload = re.sub(r\"[:,\\\"\\[\\],]\", \" \", payload)\n",
        "        payload = re.sub(r\"[^a-zA-Z0-9_\\-+*/\\\\= ]+\", \" \", payload)\n",
        "        payload = re.sub(r\"\\b0x[0-9a-fA-F]+\\b\", '', payload)\n",
        "        payload = re.sub(r'\\b[a-zA-Z]+\\s*=\\s*0x[0-9a-fA-F]+\\b', '', payload)\n",
        "        payload = re.sub(r\"=\", \"\", payload)\n",
        "        payload = re.sub(r\"[+\\-]\", \"\", payload)\n",
        "        payload = re.sub(r\"\\s+\", \" \", payload).strip()\n",
        "        if payload:\n",
        "            freq[payload] += 1\n",
        "            cleaned_payloads.append(payload)\n",
        "\n",
        "    distinct_ordered = []\n",
        "    seen = set()\n",
        "    for p in sorted(cleaned_payloads, key=lambda x: freq[x]):\n",
        "        if p not in seen:\n",
        "            seen.add(p)\n",
        "            distinct_ordered.append(p)\n",
        "    return distinct_ordered\n",
        "\n",
        "# ========== FONCTIONS D'EMBEDDING ==========\n",
        "def embed_instruction_batch(list_texts):\n",
        "    encodings = tokenizer(list_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    encodings = {k: v.to(device) for k, v in encodings.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encodings)\n",
        "        hidden = outputs.last_hidden_state\n",
        "        attention_mask = encodings[\"attention_mask\"].unsqueeze(-1).expand(hidden.size())\n",
        "        summed = (hidden * attention_mask).sum(dim=1)\n",
        "        counts = attention_mask.sum(dim=1)\n",
        "        mean_pooled = summed / counts\n",
        "    return mean_pooled.cpu()\n",
        "\n",
        "# ========== GESTION DES IDs ==========\n",
        "def get_all_embedded_ids(save_dir):\n",
        "    all_ids = []\n",
        "    for fname in os.listdir(save_dir):\n",
        "        if fname.startswith(\"embedding_\") and fname.endswith(\".pt\"):\n",
        "            try:\n",
        "                data = torch.load(os.path.join(save_dir, fname), map_location=\"cpu\")\n",
        "                all_ids.extend(data.get(\"ids\", []))\n",
        "            except:\n",
        "                continue\n",
        "    return set(all_ids)\n",
        "\n",
        "# ========== ENCODAGE PRINCIPAL ==========\n",
        "def encode_graphs_in_size_range(json_folder, save_dir, min_kb, max_kb, batch_size=32,\n",
        "                                custom_file_list=None, custom_output_name=None):\n",
        "    embedded_ids = get_all_embedded_ids(save_dir)\n",
        "\n",
        "    all_files = [(path, os.path.getsize(path) / 1024.0) for path in custom_file_list] if custom_file_list else []\n",
        "    if not custom_file_list:\n",
        "        for f in os.listdir(json_folder):\n",
        "            if not f.endswith(\".json\"): continue\n",
        "            path = os.path.join(json_folder, f)\n",
        "            size_kb = os.path.getsize(path) / 1024.0\n",
        "            if min_kb <= size_kb < max_kb:\n",
        "                graph_id = f.replace(\".json\", \"\")\n",
        "                if graph_id not in embedded_ids:\n",
        "                    all_files.append((path, size_kb))\n",
        "\n",
        "    all_files.sort(key=lambda x: x[1])\n",
        "    print(f\"Fichiers a traiter ({min_kb}-{max_kb}KB): {len(all_files)}\")\n",
        "\n",
        "    buffer_instr, buffer_ids = [], []\n",
        "    embeddings, ids, failed = [], [], []\n",
        "\n",
        "    for path, size_kb in tqdm(all_files):\n",
        "        if not is_valid_dot_file(path):\n",
        "            failed.append(os.path.basename(path))\n",
        "            continue\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                dot_text = f.read()\n",
        "            labels = extract_labels_from_text(dot_text)\n",
        "            distinct_payloads = extract_distinct_payloads_version_a(labels)\n",
        "            combined_instr = \" \".join(distinct_payloads)[:2560]\n",
        "            file_id = os.path.basename(path).replace(\".json\", \"\")\n",
        "            buffer_instr.append(combined_instr)\n",
        "            buffer_ids.append(file_id)\n",
        "\n",
        "            if len(buffer_instr) >= batch_size:\n",
        "                batch_emb = embed_instruction_batch(buffer_instr)\n",
        "                embeddings.extend(batch_emb)\n",
        "                ids.extend(buffer_ids)\n",
        "                buffer_instr.clear()\n",
        "                buffer_ids.clear()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur avec {os.path.basename(path)}: {e}\")\n",
        "            failed.append(os.path.basename(path))\n",
        "\n",
        "    if buffer_instr:\n",
        "        batch_emb = embed_instruction_batch(buffer_instr)\n",
        "        embeddings.extend(batch_emb)\n",
        "        ids.extend(buffer_ids)\n",
        "\n",
        "    if embeddings:\n",
        "        tensor = torch.stack(embeddings)\n",
        "        output_name = custom_output_name if custom_output_name else f\"embedding_{min_kb}_{max_kb}_KB.pt\"\n",
        "        torch.save({'ids': ids, 'embeddings': tensor}, os.path.join(save_dir, output_name))\n",
        "        print(f\"Sauvegarde de {len(ids)} graphs dans {output_name}\")\n",
        "\n",
        "    if failed:\n",
        "        log_path = os.path.join(save_dir, f\"log_{min_kb}_{max_kb}_KB.json\")\n",
        "        with open(log_path, \"w\") as f:\n",
        "            json.dump({\"failed\": failed, \"timestamp\": datetime.now().isoformat()}, f, indent=2)\n",
        "        failed_txt_path = os.path.join(save_dir, \"failed_graphs_remaining_test.txt\")\n",
        "        with open(failed_txt_path, \"w\") as ftxt:\n",
        "            for fname in failed:\n",
        "                ftxt.write(fname + \"\\n\")\n",
        "        print(f\"Erreurs: {len(failed)} - log dans {log_path}\")\n",
        "\n",
        "# ========== EXECUTION ==========\n",
        "if __name__ == \"__main__\":\n",
        "    max_new_pt_files = 200\n",
        "    created_pt_count = 0\n",
        "\n",
        "    embedded_ids = get_all_embedded_ids(save_dir)\n",
        "    remaining_files = []\n",
        "    for fname in os.listdir(json_folder):\n",
        "        if not fname.endswith(\".json\"):\n",
        "            continue\n",
        "        graph_id = fname.replace(\".json\", \"\")\n",
        "        if graph_id not in embedded_ids:\n",
        "            remaining_files.append(os.path.join(json_folder, fname))\n",
        "\n",
        "    print(f\"\\nGraphs restants: {len(remaining_files)}\")\n",
        "\n",
        "    categorized_files = {}\n",
        "    for lower in range(0, 100, 20):  categorized_files[f\"{lower}_{lower+20}_KB\"] = []\n",
        "    for lower in range(100, 500, 50):  categorized_files[f\"{lower}_{lower+50}_KB\"] = []\n",
        "    for lower in range(500, 1000, 50):  categorized_files[f\"{lower}_{lower+50}_KB\"] = []\n",
        "    for lower in range(1000, 10000, 1000):  categorized_files[f\"{lower}_{lower+1000}_KB\"] = []\n",
        "    for lower in range(10000, 200000, 20000):  categorized_files[f\"{lower}_{lower+20000}_KB\"] = []\n",
        "    for lower in range(200000, 400000, 50000):  categorized_files[f\"{lower}_{lower+50000}_KB\"] = []\n",
        "    categorized_files[\"over_400000_KB\"] = []\n",
        "\n",
        "    for file_path in remaining_files:\n",
        "        size_kb = os.path.getsize(file_path) / 1024.0\n",
        "        assigned = False\n",
        "        for key in categorized_files:\n",
        "            if key == \"over_400000_KB\":\n",
        "                continue\n",
        "            lower, upper, _ = key.split(\"_\")\n",
        "            if float(lower) <= size_kb < float(upper):\n",
        "                categorized_files[key].append(file_path)\n",
        "                assigned = True\n",
        "                break\n",
        "        if not assigned:\n",
        "            categorized_files[\"over_400000_KB\"].append(file_path)\n",
        "\n",
        "    for key, file_list in categorized_files.items():\n",
        "        if not file_list:\n",
        "            print(f\"Groupe {key}: aucun fichier\")\n",
        "            continue\n",
        "\n",
        "        output_name = f\"embedding_{key}.pt\"\n",
        "        output_path = os.path.join(save_dir, output_name)\n",
        "\n",
        "        if os.path.exists(output_path):\n",
        "            print(f\"Groupe {key} deja traite: {output_name}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n=== Traitement groupe {key} ({len(file_list)} fichiers) ===\")\n",
        "        min_kb = 400000 if key == \"over_400000_KB\" else int(key.split(\"_\")[0])\n",
        "        max_kb = 1_000_000 if key == \"over_400000_KB\" else int(key.split(\"_\")[1])\n",
        "\n",
        "        encode_graphs_in_size_range(\n",
        "            json_folder=json_folder,\n",
        "            save_dir=save_dir,\n",
        "            min_kb=min_kb,\n",
        "            max_kb=max_kb,\n",
        "            batch_size=32,\n",
        "            custom_file_list=file_list,\n",
        "            custom_output_name=output_name\n",
        "        )\n",
        "\n",
        "        created_pt_count += 1\n",
        "        print(f\"Groupe {key} traite et sauvegarde dans {output_name}\")\n",
        "\n",
        "        if created_pt_count >= max_new_pt_files:\n",
        "            print(\"Nombre maximal de fichiers atteint\")\n",
        "            break"
      ],
      "metadata": {
        "id": "HdYRcbDHGyTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAIN : Vérifier la base d'embedding finale - IDs manquants dans metadata"
      ],
      "metadata": {
        "id": "4JjHstOVG3He"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# ========== CONFIGURATION ==========\n",
        "embedding_dir = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/instruction_embeddings_grouped_codebert_v2\"\n",
        "metadata_csv_path = \"/content/drive/My Drive/Colab Notebooks/Data/SDC/training_set_metadata.csv\"\n",
        "\n",
        "# ========== ANALYSE DES FICHIERS .pt ==========\n",
        "pt_files = [f for f in os.listdir(embedding_dir) if f.endswith(\".pt\") and f.startswith(\"embedding_\")]\n",
        "print(f\"Total fichiers .pt: {len(pt_files)}\\n\")\n",
        "\n",
        "all_ids = set()\n",
        "duplicate_ids = set()\n",
        "\n",
        "for fname in sorted(pt_files):\n",
        "    fpath = os.path.join(embedding_dir, fname)\n",
        "    try:\n",
        "        data = torch.load(fpath, map_location=\"cpu\")\n",
        "        ids = data.get(\"ids\", [])\n",
        "        emb = data.get(\"embeddings\", None)\n",
        "\n",
        "        print(f\"Fichier {fname}:\")\n",
        "        print(f\"   - Graphs: {len(ids)}\")\n",
        "        print(f\"   - Dimensions embeddings: {emb.shape if emb is not None else 'Non trouve'}\")\n",
        "        print(f\"   - Exemples IDs: {ids[:3]}\")\n",
        "\n",
        "        for gid in ids:\n",
        "            if gid in all_ids:\n",
        "                duplicate_ids.add(gid)\n",
        "            else:\n",
        "                all_ids.add(gid)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lecture {fname}: {e}\")\n",
        "\n",
        "print(\"\\nTotal IDs uniques:\", len(all_ids))\n",
        "if duplicate_ids:\n",
        "    print(f\"IDs dupliques trouves: {len(duplicate_ids)}\")\n",
        "    print(\"   Exemples:\", list(duplicate_ids)[:10])\n",
        "else:\n",
        "    print(\"Aucun ID duplique trouve.\")\n",
        "\n",
        "# ========== LECTURE METADONNEES ==========\n",
        "print(\"\\nLecture metadata CSV...\")\n",
        "metadata_df = pd.read_csv(metadata_csv_path, sep=\";\")\n",
        "\n",
        "if 'name' not in metadata_df.columns:\n",
        "    print(\"Colonne 'name' absente. Colonnes disponibles:\", metadata_df.columns.tolist())\n",
        "else:\n",
        "    metadata_ids = set(metadata_df[\"name\"].astype(str))\n",
        "    print(f\"Total IDs dans metadata: {len(metadata_ids)}\")\n",
        "\n",
        "    # Comparaison\n",
        "    missing_from_metadata = all_ids - metadata_ids\n",
        "    print(f\"\\nIDs manquants dans metadata: {len(missing_from_metadata)}\")\n",
        "    if missing_from_metadata:\n",
        "        print(\"   Exemples:\", list(missing_from_metadata)[:10])\n",
        "    else:\n",
        "        print(\"Tous les IDs sont presents dans metadata.\")"
      ],
      "metadata": {
        "id": "MXth0891G7Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminer tous les IDs ne sont pas presents dans metadata.\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# ========== CONFIGURATION ==========\n",
        "embedding_dir = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/instruction_embeddings_grouped_codebert_v2\"\n",
        "metadata_csv_path = \"/content/drive/My Drive/Colab Notebooks/Data/SDC/training_set_metadata.csv\"\n",
        "\n",
        "# ========== LECTURE METADONNEES ==========\n",
        "metadata_df = pd.read_csv(metadata_csv_path, sep=\";\")\n",
        "metadata_ids = set(metadata_df[\"name\"].astype(str))\n",
        "print(f\"Metadata charges : {len(metadata_ids)} IDs\\n\")\n",
        "\n",
        "# ========== TRAITEMENT DES FICHIERS .pt ==========\n",
        "pt_files = [f for f in os.listdir(embedding_dir) if f.endswith(\".pt\") and f.startswith(\"embedding_\")]\n",
        "\n",
        "for fname in sorted(pt_files):\n",
        "    fpath = os.path.join(embedding_dir, fname)\n",
        "    try:\n",
        "        data = torch.load(fpath, map_location=\"cpu\")\n",
        "        ids = data.get(\"ids\", [])\n",
        "        emb = data.get(\"embeddings\", None)\n",
        "\n",
        "        if emb is None or not ids:\n",
        "            print(f\"{fname}: Aucun embedding ou ID trouve\")\n",
        "            continue\n",
        "\n",
        "        # Filtrage des IDs valides (present dans metadata)\n",
        "        valid_indices = [i for i, gid in enumerate(ids) if gid in metadata_ids]\n",
        "\n",
        "        if len(valid_indices) == len(ids):\n",
        "            print(f\"{fname}: Tous les IDs valides ({len(ids)})\")\n",
        "            continue\n",
        "\n",
        "        # Mise a jour des donnees\n",
        "        new_ids = [ids[i] for i in valid_indices]\n",
        "        new_emb = emb[valid_indices]\n",
        "        data[\"ids\"] = new_ids\n",
        "        data[\"embeddings\"] = new_emb\n",
        "\n",
        "        # Ecriture atomique\n",
        "        tmp_path = fpath + \".tmp\"\n",
        "        torch.save(data, tmp_path)\n",
        "        os.replace(tmp_path, fpath)\n",
        "\n",
        "        print(f\"{fname}: {len(ids) - len(new_ids)} IDs invalides supprimes. Reste {len(new_ids)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur traitement {fname}: {e}\")"
      ],
      "metadata": {
        "id": "-NPYjDE-H7Fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partie 2 : Feature extraction"
      ],
      "metadata": {
        "id": "iwel5xNVIMW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pour le trainset"
      ],
      "metadata": {
        "id": "Yi-QUMoOIzOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ========== CONFIGURATION ==========\n",
        "TRAIN_CSV = \"/content/drive/My Drive/Colab Notebooks/Data/SDC/training_set_metadata.csv\"\n",
        "TRAIN_GRAPH_DIR = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/SDC/Fichiers source/folder_training_set_2/folder_training_set\"\n",
        "EMBEDDING_DIR = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/instruction_embeddings_grouped_codebert_v2\"\n",
        "SAMPLE_SIZE = None\n",
        "\n",
        "# ========== FONCTIONS UTILITAIRES ==========\n",
        "def entropy(vals):\n",
        "    probs = np.array(list(vals.values())) / sum(vals.values())\n",
        "    return -np.sum(probs * np.log2(probs + 1e-9))\n",
        "\n",
        "def extract_features(filepath):\n",
        "    with open(filepath, 'r') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    inst_matches = re.findall(r'INST\\s+:\\s+([^:\\n]+)', content)\n",
        "    all_text = \" \".join(inst_matches)\n",
        "    inst_lower = [w.lower() for w in inst_matches]\n",
        "    c = Counter(inst_lower)\n",
        "\n",
        "    # Instructions communes\n",
        "    top_instrs = ['call','jmp','ret','mov','push','pop','lea']\n",
        "    stats = {f'instr_{k}': c.get(k, 0) for k in top_instrs}\n",
        "\n",
        "    # Groupes sémantiques\n",
        "    control = {'jmp','call','ret'}\n",
        "    stats['count_control'] = sum(c[i] for i in control)\n",
        "\n",
        "    # Statistiques instructions\n",
        "    stats['nb_tokens'] = len(inst_lower)\n",
        "    stats['unique_instructions'] = len(set(inst_lower))\n",
        "    stats['instruction_entropy'] = -np.sum(\n",
        "        (np.array(list(c.values())) / (len(inst_lower)+1e-9)) *\n",
        "        np.log2(np.array(list(c.values())) / (len(inst_lower)+1e-9) + 1e-9)\n",
        "    ) if inst_lower else 0\n",
        "\n",
        "    # Patterns spéciaux\n",
        "    stats['has_xor_zeroing'] = int(bool(re.search(r'xor\\s+[re]?[abcd]x,\\s+[re]?[abcd]x', all_text, re.IGNORECASE)))\n",
        "    stats['has_getproc'] = int(bool(re.search(r'getprocaddress', all_text, re.IGNORECASE)))\n",
        "\n",
        "    # Structure du graphe\n",
        "    edges = re.findall(r'\"([0-9a-fx]+)\"\\s*->\\s*\"([0-9a-fx]+)\"', content)\n",
        "    nodes = set()\n",
        "    out_degree = defaultdict(int)\n",
        "    graph = defaultdict(list)\n",
        "    for src, dst in edges:\n",
        "        nodes.add(src)\n",
        "        nodes.add(dst)\n",
        "        out_degree[src] += 1\n",
        "        graph[src].append(dst)\n",
        "\n",
        "    stats['nb_nodes'] = len(nodes)\n",
        "    stats['nb_edges'] = len(edges)\n",
        "    stats['max_out_degree'] = max(out_degree.values()) if out_degree else 0\n",
        "\n",
        "    def dfs_iterative(start_node):\n",
        "        visited = set()\n",
        "        stack = [(start_node, 0)]\n",
        "        max_depth = 0\n",
        "        while stack:\n",
        "            node, depth = stack.pop()\n",
        "            if node not in visited:\n",
        "                visited.add(node)\n",
        "                max_depth = max(max_depth, depth)\n",
        "                for child in graph.get(node, []):\n",
        "                    if child not in visited:\n",
        "                        stack.append((child, depth + 1))\n",
        "        return max_depth\n",
        "\n",
        "    entry_node = list(nodes)[0] if nodes else None\n",
        "    stats['depth_max'] = dfs_iterative(entry_node) if entry_node else 0\n",
        "\n",
        "    return all_text, stats\n",
        "\n",
        "# ========== CHARGEMENT DES IDs EMBEDDÉS ==========\n",
        "print(\"Analyse des fichiers .pt...\")\n",
        "pt_files = [f for f in os.listdir(EMBEDDING_DIR) if f.endswith(\".pt\") and f.startswith(\"embedding_\")]\n",
        "selected_ids = set()\n",
        "for fname in tqdm(pt_files):\n",
        "    fpath = os.path.join(EMBEDDING_DIR, fname)\n",
        "    try:\n",
        "        data = torch.load(fpath, map_location=\"cpu\")\n",
        "        ids = data.get(\"ids\", [])\n",
        "        selected_ids.update(ids)\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lecture {fname}: {e}\")\n",
        "print(f\"Total IDs embeddés: {len(selected_ids)}\\n\")\n",
        "\n",
        "# ========== TRAITEMENT DES GRAPHES ==========\n",
        "def process_graphs(folder, selected_ids):\n",
        "    print(f\"Traitement de {len(selected_ids)} graphes depuis: {folder}\")\n",
        "    texts, stats = {}, {}\n",
        "    for fname in tqdm(os.listdir(folder)):\n",
        "        if fname.endswith('.json'):\n",
        "            id_ = fname.replace('.json', '')\n",
        "            if id_ in selected_ids:\n",
        "                path = os.path.join(folder, fname)\n",
        "                txt, feat = extract_features(path)\n",
        "                texts[id_] = txt\n",
        "                stats[id_] = feat\n",
        "    print(f\"Extraction terminée: {len(texts)} graphes\")\n",
        "    return texts, stats\n",
        "\n",
        "# ========== CHARGEMENT DES LABELS ==========\n",
        "print(\"Chargement metadata...\")\n",
        "train_labels = pd.read_csv(TRAIN_CSV, sep=';')\n",
        "train_labels['name'] = train_labels['name'].astype(str)\n",
        "sampled = train_labels[train_labels['name'].isin(selected_ids)].copy()\n",
        "print(f\"Labels correspondants: {len(sampled)}\")\n",
        "\n",
        "# ========== EXTRACTION DES CARACTÉRISTIQUES ==========\n",
        "train_texts, train_stats = process_graphs(TRAIN_GRAPH_DIR, selected_ids)\n",
        "\n",
        "# ========== FUSION DES DONNÉES ==========\n",
        "print(\"Fusion text + stats + labels...\")\n",
        "X_text = pd.Series(train_texts).rename_axis('name').reset_index(name='text')\n",
        "X_stats = pd.DataFrame.from_dict(train_stats, orient='index').reset_index().rename(columns={'index': 'name'})\n",
        "df = pd.merge(X_text, X_stats, on='name')\n",
        "df = pd.merge(df, sampled, on='name')\n",
        "print(f\"Fusion terminée: {df.shape[0]} graphes\")"
      ],
      "metadata": {
        "id": "jbxZ-oUYIyWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pour le testset"
      ],
      "metadata": {
        "id": "eQPB0lBIJceN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import Counter, defaultdict\n",
        "import joblib\n",
        "\n",
        "# ========== CONFIGURATION ==========\n",
        "TEST_GRAPH_DIR = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/folder_test_set\"\n",
        "EMBEDDING_DIR_TEST = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/instruction_embeddings_test_codebert_v2\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/Modelisation 4/\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ========== FONCTIONS D'EXTRACTION ==========\n",
        "def extract_features(filepath):\n",
        "    with open(filepath, 'r') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    inst_matches = re.findall(r'INST\\s+:\\s+([^:\\n]+)', content)\n",
        "    all_text = \" \".join(inst_matches)\n",
        "    inst_lower = [w.lower() for w in inst_matches]\n",
        "    c = Counter(inst_lower)\n",
        "\n",
        "    top_instrs = ['call','jmp','ret','mov','push','pop','lea']\n",
        "    stats = {f'instr_{k}': c.get(k, 0) for k in top_instrs}\n",
        "\n",
        "    control = {'jmp','call','ret'}\n",
        "    stats['count_control'] = sum(c[i] for i in control)\n",
        "\n",
        "    stats['nb_tokens'] = len(inst_lower)\n",
        "    stats['unique_instructions'] = len(set(inst_lower))\n",
        "    stats['instruction_entropy'] = -np.sum(\n",
        "        (np.array(list(c.values())) / (len(inst_lower)+1e-9)) *\n",
        "        np.log2(np.array(list(c.values())) / (len(inst_lower)+1e-9) + 1e-9)\n",
        "    ) if inst_lower else 0\n",
        "\n",
        "    stats['has_xor_zeroing'] = int(bool(re.search(r'xor\\s+[re]?[abcd]x,\\s+[re]?[abcd]x', all_text, re.IGNORECASE)))\n",
        "    stats['has_getproc'] = int(bool(re.search(r'getprocaddress', all_text, re.IGNORECASE)))\n",
        "\n",
        "    edges = re.findall(r'\"([0-9a-fx]+)\"\\s*->\\s*\"([0-9a-fx]+)\"', content)\n",
        "    nodes = set()\n",
        "    out_degree = defaultdict(int)\n",
        "    graph = defaultdict(list)\n",
        "    for src, dst in edges:\n",
        "        nodes.add(src)\n",
        "        nodes.add(dst)\n",
        "        out_degree[src] += 1\n",
        "        graph[src].append(dst)\n",
        "\n",
        "    stats['nb_nodes'] = len(nodes)\n",
        "    stats['nb_edges'] = len(edges)\n",
        "    stats['max_out_degree'] = max(out_degree.values()) if out_degree else 0\n",
        "\n",
        "    def dfs_iterative(start_node):\n",
        "        visited = set()\n",
        "        stack = [(start_node, 0)]\n",
        "        max_depth = 0\n",
        "        while stack:\n",
        "            node, depth = stack.pop()\n",
        "            if node not in visited:\n",
        "                visited.add(node)\n",
        "                max_depth = max(max_depth, depth)\n",
        "                for child in graph.get(node, []):\n",
        "                    if child not in visited:\n",
        "                        stack.append((child, depth + 1))\n",
        "        return max_depth\n",
        "\n",
        "    entry_node = list(nodes)[0] if nodes else None\n",
        "    stats['depth_max'] = dfs_iterative(entry_node) if entry_node else 0\n",
        "\n",
        "    return all_text, stats\n",
        "\n",
        "# ========== CHARGEMENT DES IDs EMBEDDÉS ==========\n",
        "print(\"Recherche des fichiers .pt...\")\n",
        "pt_files = [f for f in os.listdir(EMBEDDING_DIR_TEST) if f.endswith(\".pt\") and f.startswith(\"embedding_\")]\n",
        "selected_ids = set()\n",
        "for fname in tqdm(pt_files):\n",
        "    fpath = os.path.join(EMBEDDING_DIR_TEST, fname)\n",
        "    try:\n",
        "        data = torch.load(fpath, map_location=\"cpu\")\n",
        "        ids = data.get(\"ids\", [])\n",
        "        selected_ids.update(ids)\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lecture {fname}: {e}\")\n",
        "print(f\"Total IDs embeddés (test): {len(selected_ids)}\\n\")\n",
        "\n",
        "# ========== PRÉTRAITEMENT DES GRAPHES ==========\n",
        "def process_graphs(folder, selected_ids):\n",
        "    texts, stats = {}, {}\n",
        "    json_files = [f\"{id_}.json\" for id_ in selected_ids if os.path.exists(os.path.join(folder, f\"{id_}.json\"))]\n",
        "    for fname in tqdm(json_files, desc=\"Traitement des graphes test\"):\n",
        "        id_ = fname.replace('.json', '')\n",
        "        path = os.path.join(folder, fname)\n",
        "        txt, feat = extract_features(path)\n",
        "        texts[id_] = txt\n",
        "        stats[id_] = feat\n",
        "    return texts, stats\n",
        "\n",
        "print(\"Extraction des features pour le test set...\")\n",
        "test_texts, test_stats = process_graphs(TEST_GRAPH_DIR, selected_ids)\n",
        "\n",
        "# ========== CRÉATION DU DATAFRAME ==========\n",
        "print(\"Création du DataFrame test...\")\n",
        "X_text_test = pd.Series(test_texts).rename_axis('name').reset_index(name='text')\n",
        "X_stats_test = pd.DataFrame.from_dict(test_stats, orient='index').reset_index().rename(columns={'index': 'name'})\n",
        "df_test = pd.merge(X_text_test, X_stats_test, on='name')\n",
        "print(f\"DataFrame test créé: {df_test.shape[0]} graphes.\")"
      ],
      "metadata": {
        "id": "6h_pwHrQILx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partie 3 : TF - IDF & SVD"
      ],
      "metadata": {
        "id": "FFX7A7yH2Gxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import time, psutil\n",
        "import os\n",
        "\n",
        "# ========== CONFIGURATION DES CHEMINS ==========\n",
        "SAVE_PATH = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/Modelisation 5/\"\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "\n",
        "df_test = joblib.load(os.path.join(SAVE_PATH, \"df_text_cleaned.pkl\"))\n",
        "\n",
        "# ========== PARAMETRES DU MODELE ==========\n",
        "n_components = 600\n",
        "max_features = 20000\n",
        "ngram_range = (1, 3)\n",
        "max_text_len = 200000  # Longueur maximale pour chaque échantillon\n",
        "\n",
        "def log_memory():\n",
        "    mem = psutil.virtual_memory()\n",
        "    print(f\"Memoire: {mem.used / 1e9:.2f} GB utilise / {mem.total / 1e9:.2f} GB total ({mem.percent}%)\")\n",
        "\n",
        "# ========== TRONCATURE DU TEXTE ==========\n",
        "print(f\"Troncature du texte a {max_text_len} caracteres...\")\n",
        "start = time.time()\n",
        "df['text_short'] = df['text_cleaned'].str.slice(0, max_text_len)\n",
        "print(f\"Texte traite: {len(df)} documents — temps: {time.time() - start:.2f}s\")\n",
        "log_memory()\n",
        "\n",
        "# ========== VECTORISATION TF-IDF ==========\n",
        "print(\"Initialisation du vectorizer TF-IDF...\")\n",
        "vectorizer = TfidfVectorizer(\n",
        "    ngram_range=ngram_range,\n",
        "    max_features=max_features,\n",
        "    min_df=3,\n",
        "    max_df=0.85,\n",
        "    sublinear_tf=True,\n",
        "    use_idf=True,\n",
        "    norm='l2',\n",
        "    analyzer='word',\n",
        "    stop_words=None\n",
        ")\n",
        "\n",
        "print(\"Transformation TF-IDF...\")\n",
        "start = time.time()\n",
        "X_tfidf = vectorizer.fit_transform(df['text_short'])\n",
        "print(f\"TF-IDF shape: {X_tfidf.shape} — temps: {time.time() - start:.2f}s\")\n",
        "log_memory()\n",
        "\n",
        "# ========== SAUVEGARDE ==========\n",
        "joblib.dump(vectorizer, os.path.join(SAVE_PATH, \"tfidf_vectorizer.pkl\"))\n",
        "joblib.dump(X_tfidf, os.path.join(SAVE_PATH, \"X_tfidf_raw.pkl\"))\n",
        "\n",
        "# ========== REDUCTION DE DIMENSION ==========\n",
        "print(f\"Reduction SVD ({n_components} dimensions)...\")\n",
        "start = time.time()\n",
        "svd = TruncatedSVD(n_components=n_components, random_state=42, algorithm='arpack')\n",
        "X_svd = svd.fit_transform(X_tfidf)\n",
        "print(f\"SVD shape: {X_svd.shape} — temps: {time.time() - start:.2f}s — variance: {svd.explained_variance_ratio_.sum()*100:.2f}%\")\n",
        "log_memory()"
      ],
      "metadata": {
        "id": "v0jB2OLPKLvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== MERGE SVD & df ==========\n",
        "svd_cols = [f\"svd_{i}\" for i in range(n_components)]\n",
        "df_svd = pd.DataFrame(X_svd, columns=svd_cols)\n",
        "df_svd[\"name\"] = df[\"name\"].values\n",
        "df_merged = pd.merge(df, df_svd, on=\"name\")\n",
        "\n",
        "# ========== CONCATÉNER X_final & y ==========\n",
        "embedding_cols = [col for col in df_merged.columns if col.startswith(\"emb_\")]\n",
        "stat_cols = [col for col in X_stats.columns if col != \"name\"]\n",
        "tfidf_svd_cols = svd_cols\n",
        "\n",
        "X_full_final = df_merged[embedding_cols + stat_cols + tfidf_svd_cols].values\n",
        "y_final = df_merged.drop(columns=[\"name\", \"text\", \"text_short\"] + embedding_cols + stat_cols + tfidf_svd_cols).astype(int)"
      ],
      "metadata": {
        "id": "xNRF-UYGKodK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partie 4 : Concaténer la base finale"
      ],
      "metadata": {
        "id": "jae0n2-NKsoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========== LECTURE DES FICHIERS .pt ==========\n",
        "embedding_dir = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/instruction_embeddings_grouped_codebert_v2\"\n",
        "embedding_dict = {}\n",
        "\n",
        "pt_files = [f for f in os.listdir(embedding_dir) if f.endswith(\".pt\") and f.startswith(\"embedding_\")]\n",
        "print(f\"Chargement des embeddings depuis {len(pt_files)} fichiers...\")\n",
        "\n",
        "for f in tqdm(pt_files):\n",
        "    path = os.path.join(embedding_dir, f)\n",
        "    data = torch.load(path, map_location=\"cpu\")\n",
        "    ids = data[\"ids\"]\n",
        "    embeds = data[\"embeddings\"]\n",
        "    for id_, emb in zip(ids, embeds):\n",
        "        embedding_dict[id_] = emb.numpy()\n",
        "\n",
        "print(f\"Total des graph_id avec embeddings: {len(embedding_dict)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXC8fhmDpnBw",
        "outputId": "0b99c99e-083f-4a67-aa68-b56a0db8e910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Đang load embeddings từ 150 file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:17<00:00,  8.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Tổng số graph_id có embedding: 23049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ========== CONFIGURATION ==========\n",
        "output_dir = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/Modelisation 4/\"\n",
        "embedding_dir = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/instruction_embeddings_grouped_codebert_v2\"\n",
        "\n",
        "# ========== CHARGEMENT DES DONNEES ==========\n",
        "print(\"Chargement de X_full_final et y_final...\")\n",
        "X_full = joblib.load(os.path.join(output_dir, \"X_full_final.pkl\"))  # TF-IDF + stats\n",
        "y_final = joblib.load(os.path.join(output_dir, \"y_final.pkl\"))\n",
        "print(f\"X_full shape (stats + TF-IDF): {X_full.shape}\")\n",
        "print(f\"y_final shape: {y_final.shape}\")\n",
        "\n",
        "# ========== CHARGEMENT DES EMBEDDINGS ==========\n",
        "def load_all_embeddings(embedding_dir):\n",
        "    all_rows = []\n",
        "    pt_files = [f for f in os.listdir(embedding_dir) if f.endswith(\".pt\") and f.startswith(\"embedding_\")]\n",
        "\n",
        "    print(f\"Chargement des embeddings depuis {len(pt_files)} fichiers...\")\n",
        "    for fname in tqdm(pt_files):\n",
        "        fpath = os.path.join(embedding_dir, fname)\n",
        "        try:\n",
        "            data = torch.load(fpath, map_location=\"cpu\")\n",
        "            ids = data.get(\"ids\", [])\n",
        "            embs = data.get(\"embeddings\", None)\n",
        "\n",
        "            if embs is None or len(ids) != embs.shape[0]:\n",
        "                print(f\"Fichier {fname} invalide\")\n",
        "                continue\n",
        "\n",
        "            for gid, vec in zip(ids, embs):\n",
        "                all_rows.append((gid, vec.numpy()))\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur chargement {fname}: {e}\")\n",
        "\n",
        "    print(f\"Total embeddings charges: {len(all_rows)}\")\n",
        "    return pd.DataFrame(all_rows, columns=[\"name\", \"embedding\"])\n",
        "\n",
        "# Chargement embeddings\n",
        "embedding_df = load_all_embeddings(embedding_dir)\n",
        "embedding_expanded = pd.DataFrame(embedding_df[\"embedding\"].to_list())\n",
        "embedding_expanded.columns = [f\"emb_{i}\" for i in range(embedding_expanded.shape[1])]\n",
        "embedding_expanded[\"name\"] = embedding_df[\"name\"]\n",
        "\n",
        "# ========== PREPARATION DES DONNEES ==========\n",
        "df = joblib.load(os.path.join(output_dir, \"df_merged_text_stats_labels.pkl\"))\n",
        "df = df[df[\"name\"].isin(embedding_expanded[\"name\"])].reset_index(drop=True)\n",
        "\n",
        "# ========== FUSION DES DONNEES ==========\n",
        "df_merged = pd.merge(df[[\"name\"]], embedding_expanded, on=\"name\")\n",
        "print(f\"Fusion embeddings reussie - shape: {df_merged.shape}\")\n",
        "\n",
        "# ========== CONCATENATION FINALE ==========\n",
        "X_embedding = df_merged.drop(columns=[\"name\"]).values\n",
        "X_combined = np.concatenate([X_embedding, X_full], axis=1)\n",
        "print(f\"Dimensions finales apres fusion: {X_combined.shape[1]}\")\n",
        "\n",
        "# ========== DECOUPAGE TRAIN/VAL ==========\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_combined, y_final, test_size=0.2, random_state=42)\n",
        "print(f\"X_train: {X_tr.shape}, X_val: {X_val.shape}\")\n",
        "print(f\"y_train: {y_tr.shape}, y_val: {y_val.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dh4rassfvD3b",
        "outputId": "bd53c609-3c7c-46e6-d907-9b07b4070045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Đang load X_full_final và y_final từ .pkl...\n",
            "✅ X_full shape (stats + TF-IDF): (23011, 617)\n",
            "✅ y_final shape: (23011, 453)\n",
            "📥 Đang load toàn bộ embedding từ 150 file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:01<00:00, 134.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Tổng số embedding: 23049\n",
            "🔗 Merge thành công embedding — shape: (23011, 769)\n",
            "📊 Tổng số chiều feature cuối cùng sau khi ghép embedding: 1385\n",
            "✅ X_train: (18408, 1385), X_val: (4603, 1385)\n",
            "✅ y_train: (18408, 453), y_val: (4603, 453)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partie 5 : Modélisation"
      ],
      "metadata": {
        "id": "lXhmrVyxLkIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sur le train/val 80-20 + les seuils optimisés"
      ],
      "metadata": {
        "id": "Ktw8XIgjMNJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from bayes_opt import BayesianOptimization\n",
        "except ImportError:\n",
        "    !pip install bayesian-optimization\n",
        "    from bayes_opt import BayesianOptimization\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn import set_config\n",
        "import numpy as np\n",
        "\n",
        "# ========== CONVERSION EN NUMPY ==========\n",
        "y_tr = y_tr.values if hasattr(y_tr, \"values\") else y_tr\n",
        "y_val = y_val.values if hasattr(y_val, \"values\") else y_val\n",
        "\n",
        "# ========== OPTIMISATION BAYESIENNE PAR LABEL ==========\n",
        "def optimize_thresholds_per_label(y_true, y_proba):\n",
        "    best_thresholds = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        y_true_i = y_true[:, i]\n",
        "        y_proba_i = y_proba[:, i]\n",
        "\n",
        "        def f(t):\n",
        "            y_pred_bin = (y_proba_i > t).astype(int)\n",
        "            return f1_score(y_true_i, y_pred_bin, zero_division=1)\n",
        "\n",
        "        optimizer = BayesianOptimization(f=f, pbounds={'t': (0.05, 0.95)}, random_state=42, verbose=0)\n",
        "        optimizer.maximize(init_points=5, n_iter=10)\n",
        "        best_t = optimizer.max['params']['t']\n",
        "        best_thresholds.append(best_t)\n",
        "\n",
        "    return np.array(best_thresholds)\n",
        "\n",
        "# ========== POIDS DES ECHANTILLONS ==========\n",
        "print(\"\\nCalcul des poids d'echantillons...\")\n",
        "sample_weights = np.ones(y_tr.shape[0])\n",
        "for i in range(y_tr.shape[1]):\n",
        "    col = y_tr[:, i]\n",
        "    if len(np.unique(col)) < 2:\n",
        "        continue\n",
        "    weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=col)\n",
        "    sample_weights += col * weights[1]\n",
        "\n",
        "# ========== ENTRAINEMENT DU MODELE ==========\n",
        "xgb = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    tree_method='gpu_hist',\n",
        "    reg_alpha=0.5,\n",
        "    reg_lambda=1.0,\n",
        "    gamma=0.2,\n",
        "    use_label_encoder=False,\n",
        "    verbosity=0\n",
        ")\n",
        "clf = OneVsRestClassifier(xgb, n_jobs=-1)\n",
        "\n",
        "# Configuration des poids\n",
        "set_config(enable_metadata_routing=True)\n",
        "xgb.set_fit_request(sample_weight=True)\n",
        "\n",
        "print(\"\\nEntrainement du modele XGBoost...\")\n",
        "clf.fit(X_tr, y_tr, sample_weight=sample_weights)\n",
        "\n",
        "# ========== PREDICTION ==========\n",
        "print(\"\\nPrediction sur l'ensemble de validation...\")\n",
        "y_pred_proba = clf.predict_proba(X_val)\n",
        "\n",
        "# ========== OPTIMISATION DES SEUILS PAR LABEL ==========\n",
        "print(\"\\nOptimisation des seuils par label...\")\n",
        "thresholds = optimize_thresholds_per_label(y_val, y_pred_proba)\n",
        "\n",
        "# Application des seuils\n",
        "y_pred_bin = (y_pred_proba > thresholds).astype(int)\n",
        "\n",
        "# ========== EVALUATION ==========\n",
        "macro_f1 = f1_score(y_val, y_pred_bin, average=\"macro\", zero_division=0)\n",
        "roc_auc = roc_auc_score(y_val, y_pred_proba, average=\"macro\")\n",
        "precision = precision_score(y_val, y_pred_bin, average=\"macro\", zero_division=0)\n",
        "recall = recall_score(y_val, y_pred_bin, average=\"macro\", zero_division=0)\n",
        "\n",
        "print(\"\\nResultats d'evaluation:\")\n",
        "print(f\"  • Macro F1   : {macro_f1:.4f}\")\n",
        "print(f\"  • ROC-AUC    : {roc_auc:.4f}\")\n",
        "print(f\"  • Precision  : {precision:.4f}\")\n",
        "print(f\"  • Recall     : {recall:.4f}\")\n",
        "\n",
        "# ========== LABELS LES MOINS PERFORMANTS ==========\n",
        "f1_per_label = f1_score(y_val, y_pred_bin, average=None, zero_division=1)\n",
        "worst_idx = np.argsort(f1_per_label)[:5]\n",
        "print(\"\\nTop 5 labels avec F1 le plus bas:\")\n",
        "for idx in worst_idx:\n",
        "    print(f\"   Label {idx:3d}: F1 = {f1_per_label[idx]:.4f}\")"
      ],
      "metadata": {
        "id": "SN0M4wG5Mea5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sur 100% la base de train + les seuils sauvegardés en haut"
      ],
      "metadata": {
        "id": "8HSX3wbzMgnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn import set_config\n",
        "import numpy as np\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "# ========== CONFIGURATION ==========\n",
        "output_dir = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/Modelisation 4/\"\n",
        "output_dir_1 = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/Modelisation 5/\"\n",
        "\n",
        "# ========== CHARGEMENT DES SEUILS OPTIMISES ==========\n",
        "thresholds_path = os.path.join(output_dir, \"optimized_thresholds_per_label.npy\")\n",
        "thresholds = np.load(thresholds_path)\n",
        "print(f\"Chargement de {len(thresholds)} seuils optimises depuis validation\")\n",
        "\n",
        "# ========== CHARGEMENT DES DONNEES ==========\n",
        "print(\"Chargement de X_full_final et y_final...\")\n",
        "X_full = joblib.load(os.path.join(output_dir_1, \"X_full_final.pkl\"))  # TF-IDF + stats\n",
        "y_final = joblib.load(os.path.join(output_dir_1, \"y_final.pkl\"))\n",
        "X_combined = joblib.load(os.path.join(output_dir_1, \"X_combined.pkl\"))\n",
        "print(f\"X_full shape (stats + TF-IDF): {X_full.shape}\")\n",
        "print(f\"y_final shape: {y_final.shape}\")\n",
        "\n",
        "# ========== CALCUL DES POIDS ==========\n",
        "print(\"\\nCalcul des poids d'echantillons...\")\n",
        "sample_weights = np.ones(y_final.shape[0])\n",
        "y_arr = y_final.values if hasattr(y_final, \"values\") else y_final\n",
        "\n",
        "for i in range(y_arr.shape[1]):\n",
        "    col = y_arr[:, i]\n",
        "    if len(np.unique(col)) < 2:\n",
        "        continue\n",
        "    weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=col)\n",
        "    sample_weights += col * weights[1]\n",
        "\n",
        "# ========== ENTRAINEMENT FINAL ==========\n",
        "xgb = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    tree_method='gpu_hist',\n",
        "    reg_alpha=0.5,\n",
        "    reg_lambda=1.0,\n",
        "    gamma=0.2,\n",
        "    use_label_encoder=False,\n",
        "    verbosity=0\n",
        ")\n",
        "\n",
        "clf = OneVsRestClassifier(xgb, n_jobs=-1)\n",
        "\n",
        "# Configuration des poids\n",
        "set_config(enable_metadata_routing=True)\n",
        "xgb.set_fit_request(sample_weight=True)\n",
        "\n",
        "print(\"\\nEntrainement final du modele XGBoost...\")\n",
        "clf.fit(X_combined, y_arr, sample_weight=sample_weights)\n",
        "\n",
        "# Sauvegarde du modele\n",
        "joblib.dump(clf, os.path.join(output_dir_1, \"xgb_model_full.pkl\"))\n",
        "np.save(os.path.join(output_dir_1, \"optimized_thresholds_per_label.npy\"), thresholds)\n",
        "\n",
        "print(\"\\nModele final entraine avec succes.\")\n",
        "print(\"Note: Les seuils sont conserves depuis la validation, pas de re-optimisation.\")"
      ],
      "metadata": {
        "id": "HM2YkA2bMzY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partie 6 : Prédiction sur le test"
      ],
      "metadata": {
        "id": "W4qBGZ7jNIz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Préparation la base pour le testset"
      ],
      "metadata": {
        "id": "QLbGm4VpNzun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# ========== CONFIGURATION ==========\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/Modelisation 4/\"\n",
        "OUTPUT_DIR_1 = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/Modelisation 5/\"\n",
        "EMBEDDING_DIR_TEST = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/instruction_embeddings_test_codebert_v2/\"\n",
        "TFIDF_PATH = os.path.join(OUTPUT_DIR_1, \"tfidf_vectorizer.pkl\")\n",
        "SVD_PATH = os.path.join(OUTPUT_DIR_1, \"svd_model_tfidf.pkl\")\n",
        "\n",
        "# ========== CHARGEMENT DES MODÈLES ==========\n",
        "print(\"Chargement des modèles vectorizer et SVD...\")\n",
        "vectorizer = joblib.load(TFIDF_PATH)\n",
        "svd = joblib.load(SVD_PATH)\n",
        "\n",
        "# ========== APPLICATION TF-IDF + SVD ==========\n",
        "print(\"Application TF-IDF + SVD sur l'ensemble test...\")\n",
        "df_test[\"text_short\"] = df_test[\"text_cleaned\"].str[:200000]  # Troncature pour économiser la RAM\n",
        "X_tfidf_test = vectorizer.transform(df_test[\"text_short\"])\n",
        "X_svd_test = svd.transform(X_tfidf_test)\n",
        "\n",
        "# ========== CHARGEMENT DES EMBEDDINGS TEST ==========\n",
        "def load_embeddings_test(embedding_dir):\n",
        "    rows = []\n",
        "    pt_files = [f for f in os.listdir(embedding_dir) if f.endswith(\".pt\") and f.startswith(\"embedding_\")]\n",
        "    for fname in tqdm(pt_files, desc=\"Chargement des embeddings test\"):\n",
        "        fpath = os.path.join(embedding_dir, fname)\n",
        "        data = torch.load(fpath, map_location=\"cpu\")\n",
        "        ids = data[\"ids\"]\n",
        "        embs = data[\"embeddings\"]\n",
        "        for gid, emb in zip(ids, embs):\n",
        "            rows.append((gid, emb.numpy()))\n",
        "    return pd.DataFrame(rows, columns=[\"name\", \"embedding\"])\n",
        "\n",
        "embedding_test_df = load_embeddings_test(EMBEDDING_DIR_TEST)\n",
        "embedding_test_exp = pd.DataFrame(embedding_test_df[\"embedding\"].to_list())\n",
        "embedding_test_exp.columns = [f\"emb_{i}\" for i in range(embedding_test_exp.shape[1])]\n",
        "embedding_test_final = pd.concat([embedding_test_df[\"name\"], embedding_test_exp], axis=1)\n",
        "\n",
        "# ========== FUSION DES DONNÉES ==========\n",
        "print(\"Fusion embedding + stats + tfidf_svd...\")\n",
        "df_test = df_test[df_test[\"name\"].isin(embedding_test_final[\"name\"])].reset_index(drop=True)\n",
        "df_test_merged = pd.merge(df_test, embedding_test_final, on=\"name\")\n",
        "\n",
        "# Ajout SVD\n",
        "svd_cols = [f\"svd_{i}\" for i in range(X_svd_test.shape[1])]\n",
        "df_svd_test = pd.DataFrame(X_svd_test, columns=svd_cols)\n",
        "df_svd_test[\"name\"] = df_test[\"name\"].values\n",
        "df_test_merged = pd.merge(df_test_merged, df_svd_test, on=\"name\")\n",
        "\n",
        "# ========== MATRICE FINALE TEST ==========\n",
        "embedding_cols = [col for col in df_test_merged.columns if col.startswith(\"emb_\")]\n",
        "stat_cols = [col for col in X_stats_test.columns if col != \"name\"]\n",
        "X_test_final = df_test_merged[embedding_cols + stat_cols + svd_cols].values\n",
        "\n",
        "print(f\"Matrice test finale: {X_test_final.shape}\")"
      ],
      "metadata": {
        "id": "FqvSNGmXNrp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prédiction et génération des submissions"
      ],
      "metadata": {
        "id": "2waX10NTOQSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# ========== CONFIGURATION DES CHEMINS ==========\n",
        "output_dir = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/Modelisation 4/\"\n",
        "output_dir_1 = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/Modelisation 5/\"\n",
        "submission_template_path = \"/content/drive/MyDrive/Colab Notebooks/Data/SDC/test_set_metadata_to_predict.xlsx\"\n",
        "\n",
        "# ========== CHARGEMENT DES MODELES ==========\n",
        "clf = joblib.load(os.path.join(output_dir_1, \"xgb_model_full.pkl\"))\n",
        "thresholds = np.load(os.path.join(output_dir_1, \"optimized_thresholds_per_label.npy\"))\n",
        "\n",
        "# ========== CHARGEMENT DES DONNEES TEST ==========\n",
        "X_test = joblib.load(os.path.join(output_dir_1, \"X_test_final.pkl\"))\n",
        "df_test = joblib.load(os.path.join(output_dir, \"df_test_merged_text_stats.pkl\"))  # contient les 'name' de X_test\n",
        "test_ids_predicted = df_test[\"name\"].astype(str).tolist()\n",
        "\n",
        "# ========== CHARGEMENT DU TEMPLATE ==========\n",
        "submission_df = pd.read_excel(submission_template_path)\n",
        "test_ids_full = submission_df['name'].astype(str).tolist()\n",
        "\n",
        "# ========== PREDICTION ==========\n",
        "print(\"Prediction des probabilites sur l'ensemble test...\")\n",
        "y_pred_proba = clf.predict_proba(X_test)\n",
        "y_pred = (y_pred_proba > thresholds).astype(int)\n",
        "\n",
        "# ========== CREATION DU FICHIER DE SOUMISSION ==========\n",
        "submission_array = pd.DataFrame(y_pred, columns=submission_df.columns[1:])\n",
        "submission_array.insert(0, \"name\", test_ids_predicted)\n",
        "\n",
        "# ========== GESTION DES ID MANQUANTS ==========\n",
        "missing_ids = set(test_ids_full) - set(test_ids_predicted)\n",
        "if missing_ids:\n",
        "    print(f\"Attention: {len(missing_ids)} graph_id sans embedding. Remplissage par des 0.\")\n",
        "    zero_df = pd.DataFrame(0, index=range(len(missing_ids)), columns=submission_df.columns)\n",
        "    zero_df[\"name\"] = list(missing_ids)\n",
        "    submission_array = pd.concat([submission_array, zero_df], axis=0)\n",
        "\n",
        "# ========== REORDONNANCEMENT ==========\n",
        "submission_array = submission_array.set_index(\"name\").reindex(test_ids_full).reset_index()\n",
        "\n",
        "# ========== SAUVEGARDE ==========\n",
        "submission_file = \"submission_xgb_08.xlsx\"\n",
        "submission_array.to_excel(submission_file, index=False)\n",
        "print(f\"Fichier de soumission cree: {submission_file}\")\n",
        "\n",
        "# ========== TELECHARGEMENT ==========\n",
        "files.download(submission_file)"
      ],
      "metadata": {
        "id": "wO_C92KdO9sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation par des métriques"
      ],
      "metadata": {
        "id": "PwclDBbIOd2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ===== PREDICTION SUR VALIDATION =====\n",
        "y_pred_proba = clf.predict_proba(X_val)\n",
        "y_pred_bin = (y_pred_proba > thresholds).astype(int)\n",
        "\n",
        "# ===== CALCUL DES METRIQUES PAR LABEL =====\n",
        "f1 = f1_score(y_val, y_pred_bin, average=None, zero_division=0)\n",
        "precision = precision_score(y_val, y_pred_bin, average=None, zero_division=0)\n",
        "recall = recall_score(y_val, y_pred_bin, average=None, zero_division=0)\n",
        "accuracy = np.mean(y_val == y_pred_bin, axis=0)\n",
        "\n",
        "# ===== NOMS DES LABELS =====\n",
        "label_names = y_final.columns if hasattr(y_final, \"columns\") else [f\"label_{i}\" for i in range(y_val.shape[1])]\n",
        "\n",
        "# ===== CREATION DU DATAFRAME =====\n",
        "metrics_df = pd.DataFrame({\n",
        "    \"label\": label_names,\n",
        "    \"f1_score\": f1,\n",
        "    \"precision\": precision,\n",
        "    \"recall\": recall,\n",
        "    \"accuracy\": accuracy\n",
        "}).sort_values(by=\"f1_score\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Affichage top 10\n",
        "print(\"Top 10 labels par F1-score:\")\n",
        "print(metrics_df.head(10))\n",
        "\n",
        "# ===== EXPORT EXCEL =====\n",
        "output_path = os.path.join(output_dir, \"metrics_per_label_val_05.xlsx\")\n",
        "metrics_df.to_excel(output_path, index=False)\n",
        "print(f\"Metriques par label sauvegardees dans: {output_path}\")"
      ],
      "metadata": {
        "id": "PUemyMlbO2TV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}